{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train_ZoGVYWq.csv\")\n",
    "test = pd.read_csv(\"../data/test_66516Ee.csv\")\n",
    "submission = pd.read_csv(\"../data/sample_submission_sLex1ul.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP DATA\n",
    "data = pd.concat([train, test], axis=0).reset_index(drop = True)\n",
    "data[\"sourcing_channel\"] = LabelEncoder().fit_transform(data[\"sourcing_channel\"])\n",
    "data[\"residence_area_type\"] = LabelEncoder().fit_transform(data[\"residence_area_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:train.shape[0]].reset_index(drop =True)\n",
    "test = data[train.shape[0]:].reset_index(drop =True)\n",
    "\n",
    "train_id = train[[\"id\"]]\n",
    "test_id = test[[\"id\"]]\n",
    "target = train.renewal\n",
    "\n",
    "train = train.drop([\"id\",\"renewal\"], axis=1)\n",
    "test = test.drop([\"id\",\"renewal\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Running  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "def model_lgb(x_train, x_test, y_train, y_test, test, meta_train, meta_test,train_index, test_index,fold_id):\n",
    "    dtrain = lgb.Dataset(x_train, label=y_train,categorical_feature = [\"sourcing_channel\",\"residence_area_type\"])\n",
    "    dval = lgb.Dataset(x_test, label=y_test, categorical_feature = [\"sourcing_channel\",\"residence_area_type\"])\n",
    "    params = {\n",
    "        \"nthread\":8,\n",
    "        \"bagging_fraction\": 0.85,\n",
    "        \"feature_fraction\": 0.85,\n",
    "        \"bagging_freq\": 200,\n",
    "        \"metric\":\"auc\",\n",
    "        \"objective\": \"binary\",\n",
    "        \"learning_rate\":0.15,\n",
    "        \"num_leaves\":5,\n",
    "        \"max_depth\":5,\n",
    "        \"silent\":-1,\n",
    "        \"verbose\":-1,\n",
    "        \"bagging_seed\" : 98777,\n",
    "        \"seed\":87878\n",
    "    }\n",
    "    model = lgb.train(params, dtrain, num_boost_round=5000,valid_sets=[dtrain, dval], early_stopping_rounds=200, verbose_eval=100)\n",
    "    meta_train[test_index] = model.predict(x_test, num_iteration=model.best_iteration or 5000)\n",
    "    meta_test.append(model.predict(test, num_iteration=model.best_iteration or 5000))\n",
    "    \n",
    "    global meta_train_tid\n",
    "    meta_train_tid[train_index] = model.predict(x_train, num_iteration=model.best_iteration or 5000)\n",
    "\n",
    "    # Calculate Feature Importance\n",
    "    global feature_importance\n",
    "    gain = model.feature_importance('gain')\n",
    "    feature_importance = feature_importance.append(pd.DataFrame({'feature':model.feature_name(), 'split':model.feature_importance('split'), 'gain':100 * gain / gain.sum()}), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:657: UserWarning: silent keyword has been found in `params` and will be ignored. Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:681: UserWarning: categorical_feature in param dict is overrided.\n",
      "  warnings.warn('categorical_feature in param dict is overrided.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.849933\tvalid_1's auc: 0.849954\n",
      "[200]\ttraining's auc: 0.857864\tvalid_1's auc: 0.849719\n",
      "[300]\ttraining's auc: 0.865445\tvalid_1's auc: 0.849685\n",
      "Early stopping, best iteration is:\n",
      "[180]\ttraining's auc: 0.855957\tvalid_1's auc: 0.850339\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.851467\tvalid_1's auc: 0.846278\n",
      "[200]\ttraining's auc: 0.859962\tvalid_1's auc: 0.844865\n",
      "Early stopping, best iteration is:\n",
      "[96]\ttraining's auc: 0.851166\tvalid_1's auc: 0.846385\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.853945\tvalid_1's auc: 0.833477\n",
      "[200]\ttraining's auc: 0.860891\tvalid_1's auc: 0.834505\n",
      "[300]\ttraining's auc: 0.866964\tvalid_1's auc: 0.833552\n",
      "[400]\ttraining's auc: 0.872863\tvalid_1's auc: 0.833076\n",
      "Early stopping, best iteration is:\n",
      "[240]\ttraining's auc: 0.863314\tvalid_1's auc: 0.834895\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.854456\tvalid_1's auc: 0.835798\n",
      "[200]\ttraining's auc: 0.862369\tvalid_1's auc: 0.836071\n",
      "[300]\ttraining's auc: 0.869125\tvalid_1's auc: 0.834972\n",
      "Early stopping, best iteration is:\n",
      "[145]\ttraining's auc: 0.858215\tvalid_1's auc: 0.836338\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.852269\tvalid_1's auc: 0.842493\n",
      "[200]\ttraining's auc: 0.860526\tvalid_1's auc: 0.842411\n",
      "[300]\ttraining's auc: 0.868454\tvalid_1's auc: 0.840479\n"
     ]
    }
   ],
   "source": [
    "meta_train = np.zeros(train.shape[0])\n",
    "meta_train_tid = np.zeros(train.shape[0])\n",
    "meta_test = []\n",
    "feature_importance = pd.DataFrame(columns = [\"feature\",\"split\",\"gain\"])\n",
    "\n",
    "kf = StratifiedKFold(n_splits= 5, shuffle=True, random_state=3353)\n",
    "for fold_id, (train_index, test_index) in enumerate(kf.split(train, target)):\n",
    "    x_train, x_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "\n",
    "    model_lgb(x_train, x_test, y_train, y_test, test, meta_train, meta_test,train_index, test_index,fold_id)\n",
    "\n",
    "print \"Overall Score:\", roc_auc_score(target, meta_train)\n",
    "\n",
    "test_id[\"renewal\"] = pd.DataFrame(np.array(meta_test).T).rank(pct = True, axis=0).mean(axis=1)\n",
    "train_id[\"renewal\"] = meta_train\n",
    "train_id[\"renewal_tid\"] = meta_train_tid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Feature Importance\n",
    "feature_importance = feature_importance.groupby(\"feature\")[\"gain\"].mean().reset_index().sort_values('gain', ascending=False).reset_index(drop=True)\n",
    "plt.figure()\n",
    "feature_importance[['feature','gain']].head(60).plot(kind='barh', x='feature', y='gain', legend=False, figsize=(20, 10));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on Incentives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effort(incentive):\n",
    "    return 10*(1-np.exp(-incentive/400.0))\n",
    "    \n",
    "def get_improvement_in_renewal_probability(incentive):\n",
    "    effort = get_effort(incentive)\n",
    "    return 20*(1-np.exp(-effort/5.0))\n",
    "\n",
    "def gradient_improvement_in_renewal_probability(incentive):\n",
    "    dpde = 4 * np.exp(-get_effort(incentive)/5.0)\n",
    "    dedi = np.exp(-incentive/400.0)/40.0\n",
    "    dpdi = dpde * dedi\n",
    "    return  dpdi\n",
    "    \n",
    "def revenue_score(incentive, benchmark, premium):\n",
    "    cdp = get_improvement_in_renewal_probability(incentive)\n",
    "    profits = ((benchmark + (cdp*benchmark/100.0)) * premium)-incentive\n",
    "    return np.sum(profits)\n",
    "\n",
    "def score_gradient(incentive, benchmark, premium):\n",
    "    z=  np.exp(-incentive/400.0)-(incentive/400.0)-2\n",
    "    return ((benchmark * premium * np.exp(z)/400.0)-1)\n",
    "\n",
    "def second_score_gradient(incentive, benchmark, premium):\n",
    "    a = np.exp(-incentive/400.0)-(incentive/400.0)\n",
    "    b = (benchmark* premium* np.exp(a))/400.0\n",
    "    return (-(b)-(1.0/400.0))/400.0\n",
    "\n",
    "def learn(benchmark, premium, early_stopping = 5, start_incentive = 1700, min_improvement = 0.005, verbose = True):\n",
    "    incentive = start_incentive\n",
    "    best_incentive = start_incentive\n",
    "    \n",
    "    score = 0\n",
    "    best_score = 0\n",
    "    \n",
    "    counter = 0\n",
    "    best_counter = 0\n",
    "    no_improvement_counter = 0\n",
    "    \n",
    "    while True:\n",
    "        lr = (incentive - 0)\n",
    "        gradient = score_gradient(incentive, benchmark, premium)\n",
    "        try:\n",
    "            incentive += lr*(gradient*0.5 + 0.5*prev_gradient)\n",
    "        except:\n",
    "            incentive += lr*gradient\n",
    "            \n",
    "        \n",
    "        score = revenue_score(incentive, benchmark, premium)\n",
    "        counter +=1\n",
    "        prev_gradient = gradient\n",
    "\n",
    "        if (score-best_score> min_improvement):\n",
    "            best_score = score\n",
    "            best_incentive = incentive\n",
    "            best_counter = counter\n",
    "            \n",
    "            no_improvement_counter = 0\n",
    "            if verbose: print \"Epoch: {}, Incentive: {}, Score: {}, Gradient: {}\".format(counter,incentive, score, gradient)\n",
    "        else:\n",
    "            if (no_improvement_counter > early_stopping):\n",
    "                if verbose: print \"Early Stopping, Best Iteration Round: {}\".format(best_counter)\n",
    "                return best_incentive, best_score\n",
    "            else:\n",
    "                no_improvement_counter +=1\n",
    "                if verbose: print \"Epoch: {}, Incentive: {}, Score: {}, Gradient: {}\".format(counter,incentive, score, gradient)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annealing(benchmark, premium, num_annealing = 30):\n",
    "    results = []\n",
    "    for incentive in range(0,premium,int(float(premium)/num_annealing)):\n",
    "        results.append(learn(benchmark, premium, early_stopping = 5, start_incentive = incentive, min_improvement = 0.005, verbose = False))\n",
    "    return sorted(results, key = lambda x: x[1])[-1][0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_incentives = []\n",
    "\n",
    "for i, (benchmark, premium) in enumerate(zip(train_id[\"renewal\"], train[\"premium\"])):\n",
    "    if i%10000 == 0: print \"Completed {} Rows\".format(i)\n",
    "    train_incentives.append(annealing(benchmark, premium))\n",
    "    \n",
    "train_incentives = pd.Series(train_incentives)\n",
    "train_incentives[train_incentives<0] = 0\n",
    "\n",
    "generated_score = revenue_score(train_incentives, train_id[\"renewal\"], train[\"premium\"])\n",
    "baseline_score = revenue_score(pd.Series([1650]*len(train_incentives)), train_id[\"renewal\"], train[\"premium\"])\n",
    "\n",
    "print \"Score using generated Incentive:\",generated_score\n",
    "print \"Score using 1650 as Incentive:\",baseline_score\n",
    "print \"Score Improvement:\", generated_score/baseline_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_incentives = []\n",
    "\n",
    "for i, (benchmark, premium) in enumerate(zip(test_id[\"renewal\"], test[\"premium\"])):\n",
    "    if i%10000 == 0: print \"Completed {} Rows\".format(i)\n",
    "    test_incentives.append(annealing(benchmark, premium))\n",
    "    \n",
    "test_incentives = pd.Series(test_incentives)\n",
    "test_incentives[test_incentives<0] = 0\n",
    "\n",
    "generated_score = revenue_score(test_incentives, test_id[\"renewal\"], test[\"premium\"])\n",
    "baseline_score = revenue_score(pd.Series([1650]*len(test_incentives)), test_id[\"renewal\"], test[\"premium\"])\n",
    "\n",
    "print \"Score using generated Incentive:\",generated_score\n",
    "print \"Score using 1650 as Incentive:\",baseline_score\n",
    "print \"Score Improvement:\", generated_score/baseline_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training for Incentives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def model_lgb_regression(x_train, x_test, y_train, y_test, test, meta_train, meta_test,train_index, test_index,fold_id):\n",
    "    dtrain = lgb.Dataset(x_train, label=y_train)\n",
    "    dval = lgb.Dataset(x_test, label=y_test)\n",
    "    params = {\n",
    "        \"nthread\":8,\n",
    "        \"bagging_fraction\": 0.85,\n",
    "        \"feature_fraction\": 0.85,\n",
    "        \"bagging_freq\": 200,\n",
    "        \"metric\":\"rmse\",\n",
    "        \"objective\": \"regression\",\n",
    "        \"learning_rate\":0.05,\n",
    "        \"num_leaves\":10,\n",
    "        \"max_depth\":10,\n",
    "        \"silent\":-1,\n",
    "        \"verbose\":-1,\n",
    "        \"bagging_seed\" : 2425,\n",
    "        \"seed\":4244\n",
    "    }\n",
    "    model = lgb.train(params, dtrain, num_boost_round=5000,valid_sets=[dtrain, dval], early_stopping_rounds=200, verbose_eval=100)\n",
    "    meta_train[test_index] = model.predict(x_test, num_iteration=model.best_iteration or 5000)\n",
    "    meta_test.append(model.predict(test, num_iteration=model.best_iteration or 5000))\n",
    "\n",
    "    # Calculate Feature Importance\n",
    "    global feature_importance\n",
    "    gain = model.feature_importance('gain')\n",
    "    feature_importance = feature_importance.append(pd.DataFrame({'feature':model.feature_name(), 'split':model.feature_importance('split'), 'gain':100 * gain / gain.sum()}), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"prediction\"] = train_id[\"renewal\"]\n",
    "test[\"prediction\"] = train_id[\"renewal\"]\n",
    "\n",
    "meta_train = np.zeros(train.shape[0])\n",
    "meta_test = []\n",
    "feature_importance = pd.DataFrame(columns = [\"feature\",\"split\",\"gain\"])\n",
    "\n",
    "kf = StratifiedKFold(n_splits= 5, shuffle=True, random_state=24453)\n",
    "for fold_id, (train_index, test_index) in enumerate(kf.split(train, target)):\n",
    "    x_train, x_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    y_train, y_test = train_incentives[train_index], train_incentives[test_index]\n",
    "\n",
    "    model_lgb_regression(x_train, x_test, y_train, y_test, test, meta_train, meta_test,train_index, test_index,fold_id)\n",
    "\n",
    "print \"Overall Score:\", mean_squared_error(train_incentives, meta_train)\n",
    "test_id[\"incentives\"] = np.array(meta_test).T.mean(axis=1)\n",
    "train_id[\"incentives\"] = meta_train\n",
    "\n",
    "test_id[\"incentives\"] += test_incentives\n",
    "test_id[\"incentives\"] /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Feature Importance\n",
    "feature_importance = feature_importance.groupby(\"feature\")[\"gain\"].mean().reset_index().sort_values('gain', ascending=False).reset_index(drop=True)\n",
    "plt.figure()\n",
    "feature_importance[['feature','gain']].head(60).plot(kind='barh', x='feature', y='gain', legend=False, figsize=(20, 10));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission[[\"id\"]].merge(test_id[[\"id\",\"renewal\",\"incentives\"]], how = \"left\", on = \"id\")\n",
    "submission = submission[[\"id\", \"renewal\", \"incentives\"]]\n",
    "submission.loc[submission.incentives < 0,\"incentives\"] = 0\n",
    "submission.to_csv(\"csv/eda6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
